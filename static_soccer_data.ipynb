{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF2pfK2W64D5"
      },
      "source": [
        "# Static Web Scraping for Sports Data\n",
        "In this notebook, we will use several Python libraries to scrape static data from the Premier League's official site. More specifically, we will scrape data about the current standings in the league, creating a dataset that would allow us to proceed down the data science pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is an image of the site we are going to scrape. Essentially, we will interact with the site to get the data shown in the following table into a format in Python that we can operate on. (Note that only the top of the table is shown, and that Chelsea isn't there)\n",
        "\n",
        "![Image of current table on premier league site](images/pl-tables.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsg24fEaHQ29"
      },
      "source": [
        "Before we start, we will import the relevant Python libraries, requests, Beautiful Soup 4, and Pandas. This is done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.28.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2022.12.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\tyler\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install requests\n",
        "%pip install beautifulsoup4\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYI5XhEFHbBq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLFgyrhp6u8D"
      },
      "source": [
        "First, we will use the Requests library to make an HTTP get request to the link that has the data we are trying to scrape. This will return the HTML content of the page, which we can parse through in later steps.\n",
        "\n",
        "Note how we separate base_url the tables_path, when the url we are going to visit is a concatenation of the two (https://www.premierleague.com/). In a scenario where we have many different urls to visit that all use a single base url, this can be a simple way to keep track of and store these urls, avoiding redundancy from storing the base_url with every possible path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkiSlpZM6nxE"
      },
      "outputs": [],
      "source": [
        "# URL to request\n",
        "base_url = \"https://www.premierleague.com/\"\n",
        "tables_path = \"tables\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8JexhVyI-Kh"
      },
      "outputs": [],
      "source": [
        "# make get request\n",
        "response = requests.get(base_url + tables_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QqQsS2n79YB"
      },
      "source": [
        "We can check the status code below. A status code 200 means that the request was successful; in our case, it means that we were successfully able to access the web page and pull its HTML content. We cam use an assertion to formally verify this. We will get an assertion error if the request fails (i.e., it is not 200). This can help us catch problems early on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkBPeN-zPHkS",
        "outputId": "410f3ec7-aee0-45ec-c8e4-5958c87126e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response okay? True\n",
            "Status code: 200\n"
          ]
        }
      ],
      "source": [
        "assert response.status_code == 200\n",
        "\n",
        "print(f\"Response okay? {response.ok}\")\n",
        "print(f\"Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFg05fdC8PXA"
      },
      "source": [
        "We check the content type of the response below. As we can see, we get the HTML of the page returned in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOgl30S0PoB_",
        "outputId": "5e03e9a2-b8a0-44d1-ed74-261383001a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text/html;charset=utf-8\n"
          ]
        }
      ],
      "source": [
        "print(response.headers['Content-Type'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKltZ9E28WNY"
      },
      "source": [
        "We can set up a BeautifulSoup4 object passing in html.parser to parse the HTML response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZ0I0gBQAD-"
      },
      "outputs": [],
      "source": [
        "soup = bs(response.content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPCn-nqs8bfT"
      },
      "source": [
        "Here, we create a list for each column. This is where we will place data that we parse, and will allow us to easily pass it into a Pandas dataframe object later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM69XcRkRv5b"
      },
      "outputs": [],
      "source": [
        "club = []\n",
        "position = []\n",
        "prev_position = []\n",
        "played = []\n",
        "won = []\n",
        "drawn = []\n",
        "lost = []\n",
        "gf = []\n",
        "ga = []\n",
        "gd = []\n",
        "points = []\n",
        "\n",
        "col_names = [\"club\", \"position\", \"prev_position\", \"played\", \"won\", \"drawn\", \"lost\", \"gf\", \"ga\", \"gd\", \"points\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM0Y6ynrWGQE"
      },
      "source": [
        "Let's use our BeautifulSoup object to explore a few ways of retrieving data, and then we can scrape the data we need.\n",
        "\n",
        "The first way is with the find_all() function in BeautifulSoup4. This allows us to find HTML elements in the DOM according to element types and other information associated with these elements. Some of these include regex, lists that specify multiple items to match with, or even using a user-defined function. Read more about those here: https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suDJUN3xWFya",
        "outputId": "6148e5de-96c7-4739-db5d-c775078b733f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Skip to main navigation, #mainNav\n",
            "\n",
            "Text: Skip to main content, #mainContent\n",
            "\n",
            "Text: Arsenal\n",
            "\n",
            "Arsenal, http://www.arsenal.com?utm_source=premier-league-website&utm_campaign=website&utm_medium=link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Finding all a tags in the html using find_all then getting their links\n",
        "a_tags = soup.find_all(\"a\")\n",
        "\n",
        "# Print out the text and the link for the first three\n",
        "for tag in a_tags[:3]:\n",
        "  print(f\"Text: {tag.text.strip()}, {tag['href'].strip()}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1-l7f9Uuw1s"
      },
      "source": [
        "Below we define a function that takes in a tag and returns whether the tag has the href attribute defined and if it does not have an id defined. When we pass this into the find_all() function, we can find all the elements that match this criteria. We can list the number of elements that match this below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R7uoYyouBES",
        "outputId": "5e889227-efad-46a0-ff0c-e1f6b20d8884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1061\n"
          ]
        }
      ],
      "source": [
        "# Function to return if an element has both an href defined and an id defined\n",
        "def link_with_no_id(tag):\n",
        "    return tag.has_attr('href') and not tag.has_attr('id')\n",
        "\n",
        "print(len((soup.find_all(link_with_no_id))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQ7ZllxtOUE"
      },
      "source": [
        "Another method we can use is with the select() function in BeautifulSoup4. This is similar to find_all() in terms of functionality, but it instead uses the CSS selector defined in the HTML document. Both can be useful; find_all() is generally faster, while select() can be useful for complex relationships between elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj8SbJ7jsXap",
        "outputId": "4901692b-a8b9-49b1-8573-f2c29cc8a5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: , https://www.premierleague.com/home\n",
            "\n",
            "Text: Liverpool, /clubs/10/Liverpool/overview\n",
            "\n",
            "Text: LIV\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3 -1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BUR, /match/93554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Another method: using select to get all a tags where the parent is a div\n",
        "css_selector = \"div > a\"\n",
        "selector_res = soup.select(css_selector)\n",
        "\n",
        "for tag in selector_res[:3]:\n",
        "  print(f\"Text: {tag.text.strip()}, {tag['href'].strip()}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHHgJOybwwqU"
      },
      "source": [
        "Next, we will gather our data. We will first attempt to find all of the club names. From the chart, we know there are 20 teams in the Premier League. In the browser, we can inspect element and see that the name is in a span with a specific combination of CSS classes. We can use the class_ argument in find_all to try to find matching elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMYCmcc9UNHX",
        "outputId": "c04e138c-143f-4bcc-da78-95261126707f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First attempt number of elements with team names: 72\n"
          ]
        }
      ],
      "source": [
        "club_name_long = [i.text for i in soup.find_all(\"span\", class_=\"league-table__team-name league-table__team-name--long long\")]\n",
        "print(f\"Number of elements with team names: {len(club_name_long)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2041p2xpnY"
      },
      "source": [
        "Is the above expected? No. We expect 20 teams, not 72. Therefore, we need to be more specific, or try a new method. Here, we will try a different method using selectors. Using the document again, we can find that the parent table has a class .isPL that we can use in conjunction with the classes from the previous attempt as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz8nUf8yxhQY",
        "outputId": "04499c08-1e3a-40a0-fc71-81ed11490815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Second attempt number of elements with team names: 20\n",
            "Using selector:  ['Liverpool', 'Manchester City', 'Arsenal', 'Tottenham Hotspur', 'Aston Villa', 'Manchester United', 'Newcastle United', 'West Ham United', 'Brighton and Hove Albion', 'Chelsea', 'Wolverhampton Wanderers', 'Fulham', 'Bournemouth', 'Brentford', 'Crystal Palace', 'Nottingham Forest', 'Luton Town', 'Everton', 'Burnley', 'Sheffield United']\n"
          ]
        }
      ],
      "source": [
        "selector = \".isPL .league-table__team .league-table__team-name--long\"\n",
        "club_name_long = [i.text for i in soup.select(selector)]\n",
        "print(f\"Second attempt number of elements with team names: {len(club_name_long)}\")\n",
        "print(\"Using selector: \", club_name_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYV92xjUyZyD"
      },
      "source": [
        "The above was what we were looking for, yielding 20 elements. Note the difference between the approaches used: in find_all(), the class we specify is for a specific element, whereas in select(), our structure represents a hierarchical structure (for example, class isPL that is a parent of league-table__team. etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufI_A6aZymug"
      },
      "source": [
        "Another way we can find the elements is simply taking the index of the elements we need from a more general term. Therefore, we can use the first example and take the first 20 with indexing, as we can show with some print statements that these are the elements we are looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvno4WglyXeK",
        "outputId": "021e1489-9847-4c93-8265-b6cff66b8ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Third attempt number of elements with team names: 20\n",
            "Using find_all() with indexing:  ['Liverpool', 'Manchester City', 'Arsenal', 'Tottenham Hotspur', 'Aston Villa', 'Manchester United', 'Newcastle United', 'West Ham United', 'Brighton and Hove Albion', 'Chelsea', 'Wolverhampton Wanderers', 'Fulham', 'Bournemouth', 'Brentford', 'Crystal Palace', 'Nottingham Forest', 'Luton Town', 'Everton', 'Burnley', 'Sheffield United']\n"
          ]
        }
      ],
      "source": [
        "club_name_long_index = [i.text for i in soup.find_all(\"span\", class_=\"league-table__team-name league-table__team-name--long long\")][:20]\n",
        "print(f\"Third attempt number of elements with team names: {len(club_name_long_index)}\")\n",
        "print(\"Using find_all() with indexing: \", club_name_long_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF30ozG3177e"
      },
      "source": [
        "As we can see, the third method is correct as well. However, the selector method is preferred because the element structure in the find_all() example is clearly used a lot. There is a chance that the page could restructure and suddenly, the first 20 elements aren't what we want. Page restructuring is a potential problem for all methods, but if we are using a general selector that could be added in other places in the DOM in future site updates, our code is more likely to break in future additions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3Xi6e-12cN_"
      },
      "source": [
        "Below, we will go through and gather the rest of the data we need. Since we will be using generally the same method to gather each column of data, we can use a function called table_extract to avoid redundancies. It takes in a subclass that is some sort of descendant from .isPL, a class that the main table has. It then uses a similar method to how we gathered club_names_long to gather data. It also takes in a data type parameter data_type that allows us to cast the data to a particular type and return it in the proper format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8hNMeosHANT"
      },
      "outputs": [],
      "source": [
        "def table_extract(selector: str, data_type: type = str) -> list:\n",
        "  new_selector = \".isPL \" + selector\n",
        "  data = [data_type(i.text.strip()) for i in soup.select(new_selector)]\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp9i_i6NSfmp"
      },
      "outputs": [],
      "source": [
        "prev_position = table_extract(\".league-table__pos .league-table__result-highlight\", int)\n",
        "played = table_extract(\"tr > td:nth-child(3)\", int)\n",
        "won = table_extract(\"tr > td:nth-child(4)\", int)\n",
        "drawn = table_extract(\"tr > td:nth-child(5)\", int)\n",
        "lost = table_extract(\"tr > td:nth-child(6)\", int)\n",
        "gf = table_extract(\"tr > td:nth-child(7)\", int)\n",
        "ga = table_extract(\"tr > td:nth-child(8)\", int)\n",
        "gd = table_extract(\"tr > td:nth-child(9)\", int)\n",
        "points = table_extract(\"tr > td:nth-child(10)\", int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vp-MeBg4KZc"
      },
      "source": [
        "Now that we have our data in lists, we can use the pandas library to add the data to a dataframe, a convenient and useful way of storing data for efficient operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVNPPHjjSuzU",
        "outputId": "0b673f8f-c13a-4579-da13-a4cff768528e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        club  position  prev_position  played  won  drawn  \\\n",
            "0                  Liverpool         1              1      24   16      6   \n",
            "1            Manchester City         2              2      23   16      4   \n",
            "2                    Arsenal         3              3      24   16      4   \n",
            "3          Tottenham Hotspur         4              5      24   14      5   \n",
            "4                Aston Villa         5              4      24   14      4   \n",
            "5          Manchester United         6              6      24   13      2   \n",
            "6           Newcastle United         7              9      24   11      3   \n",
            "7            West Ham United         8              7      24   10      6   \n",
            "8   Brighton and Hove Albion         9              8      24    9      8   \n",
            "9                    Chelsea        10             11      24   10      4   \n",
            "10   Wolverhampton Wanderers        11             10      24    9      5   \n",
            "11                    Fulham        12             13      24    8      5   \n",
            "12               Bournemouth        13             12      23    7      6   \n",
            "13                 Brentford        14             15      23    7      4   \n",
            "14            Crystal Palace        15             14      24    6      6   \n",
            "15         Nottingham Forest        16             16      24    5      6   \n",
            "16                Luton Town        17             17      23    5      5   \n",
            "17                   Everton        18             18      24    8      5   \n",
            "18                   Burnley        19             19      24    3      4   \n",
            "19          Sheffield United        20             20      24    3      4   \n",
            "\n",
            "    lost  gf  ga  gd  points  \n",
            "0      2  55  23  32      54  \n",
            "1      3  56  25  31      52  \n",
            "2      4  53  22  31      52  \n",
            "3      5  51  36  15      47  \n",
            "4      6  50  32  18      46  \n",
            "5      9  33  33   0      41  \n",
            "6     10  51  39  12      36  \n",
            "7      8  36  42  -6      36  \n",
            "8      7  43  40   3      35  \n",
            "9     10  41  40   1      34  \n",
            "10    10  37  39  -2      32  \n",
            "11    11  33  39  -6      29  \n",
            "12    10  31  44 -13      27  \n",
            "13    12  34  39  -5      25  \n",
            "14    12  27  43 -16      24  \n",
            "15    13  30  44 -14      21  \n",
            "16    13  33  45 -12      20  \n",
            "17    11  26  32  -6      19  \n",
            "18    17  25  50 -25      13  \n",
            "19    17  22  60 -38      13  \n"
          ]
        }
      ],
      "source": [
        "club = club_name_long\n",
        "position = [i for i in range(1, 21)]\n",
        "\n",
        "cols = [club, position, prev_position, played, won, drawn, lost, gf, ga, gd, points]\n",
        "\n",
        "data = dict()\n",
        "for i in range(len(col_names)):\n",
        "    data[col_names[i]] = cols[i]\n",
        "\n",
        "df = pd.DataFrame(data=data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvLR2LR5TPmM"
      },
      "source": [
        "As we can see, we now have our data stored in a dataframe that can be used in further steps in the data science pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwZ-ZsgSHoIj"
      },
      "source": [
        "For more information about the libraries used here, visit the docs:\n",
        "\n",
        "Python libraries discussed:\n",
        "- requests: https://requests.readthedocs.io/en/latest/\n",
        "- Beautiful Soup 4: https://beautiful-soup-4.readthedocs.io/en/latest/\n",
        "- pandas: https://pandas.pydata.org/docs/\n",
        "\n",
        "Other useful links:\n",
        "- Document Object Model (DOM) https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model\n",
        "- HTTP: https://www.ibm.com/docs/en/cics-ts/5.3?topic=protocol-http-requests"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
