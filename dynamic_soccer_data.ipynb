{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Web Scraping for Sports Data\n",
    "In this notebook, we will expand on the concepts used in the static web scraping notebook. We will still be using the Premier League's official site, this time scraping all of the current game results for this season. We have some added complexity this time that makes dynamic web scraping necessary to have access to the data we want.\n",
    "\n",
    "The site we are working with is shown below. This page uses lazy loading, a principle that ensures that content is only loaded when a user requests it. In this case, \"requesting\" the data means scrolling down enough so that the end of the currently loaded content is reached. However, we want all the content in our dataset, and we will automate web actions with Selenium to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pl-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library we will be using is Selenium. This library is great with web automation, making it an option for many use cases, from anything such as web scraping (our use case) to website testing. Install Selenium below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install beautifulsoup4\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference that sets Selenium apart from BeautifulSoup is the static vs. dynamic nature of the web scraping we are going to be doing. With BeautifulSoup, we retrieve HTML content from a page at a snapshot in time that we pass into a BeautifulSoup object, where it is then parsed.  After we make the get request, that HTML object is not going to automatically change; it's static. On the other hand, with Selenium, we use a webdriver to start an instance of a web browser. This API gives us dynamic access to the browser, meaning that we can interact with the site as a human might in realtime. In our use case, we will be approaching a simple but crucial problem: our data will not all load until we click away several elements and scroll down on the page. Once we do this, we can simply retrieve the HTML of the page, and then use BeautifulSoup as we did in the last example.\n",
    "\n",
    "We will be using the webdriver and Service APIs from Selenium to start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Premier League's results page to make a simple dataset that has information on every match in the leauge so far this season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_url = \"https://www.premierleague.com/results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below creates a webdriver, which gives us a programmatic entrypoint into the web browser. \n",
    "\n",
    "The code below uses Chrome, though other browsers can be used (for more information: https://selenium-python.readthedocs.io/installation.html#drivers)\n",
    "\n",
    "*Note: in order to run the code, your ChromeDriver version must be identical to the version of Chrome installed on your device.*\n",
    "\n",
    "To download ChromeDriver:\n",
    "- Chrome version <=114>: https://chromedriver.chromium.org/downloads\n",
    "- Chrome version >114: https://googlechromelabs.github.io/chrome-for-testing/\n",
    "\n",
    "In the repository, I currently have ChromeDriver version 121.X.XXXX.XX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a webdriver.Chrome instance using the ./chromedriver.exe path; therefore, place your chromedriver.exe file in the base directory of this project. An example chromedriver.exe file is already present, but ensure you use the correct version to avoid errors.\n",
    "\n",
    "We also have the concept of headless below: currently, when we run our webdriver, it will open with a visible and interactive version of the browser. Headless allows the chrome instance to run without a UI. This can cause some differences in functionality at times, and in this case we will keep headless off. However, it is a useful feature to allow web automation tasks to run in the background, and to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver(headless: bool = False):\n",
    "    # Path to the chromedriver executable\n",
    "    chromedriver_path = './chromedriver.exe'\n",
    "\n",
    "    # Set headless mode if specified\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "\n",
    "    # Start and return the chrome insance\n",
    "    return webdriver.Chrome(service=Service(executable_path=chromedriver_path), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple instance of initializing a driver, making a get request, and then quitting the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "\n",
    "# Navigate to the url\n",
    "driver.get(results_url)\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Class\n",
    "Using the \"By\" class, we can access elements by many different methods:\n",
    "- ID = \"id\"\n",
    "- NAME = \"name\"\n",
    "- XPATH = \"xpath\"\n",
    "- LINK_TEXT = \"link text\"\n",
    "- PARTIAL_LINK_TEXT = \"partial link text\"\n",
    "- TAG_NAME = \"tag name\"\n",
    "- CLASS_NAME = \"class name\"\n",
    "- CSS_SELECTOR = \"css selector\"\n",
    "\n",
    "This class is used in conjunction with the find_element() and find_elements() APIs to give us different ways of specifying criteria in which to look for elements in the dynamic DOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will use the find_element() and click() APIs. The click() function does exactly what you might expect - given an element, it will perform a click on it. The code below closes two popups that show up when you enter the site without any cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver()\n",
    "driver.get(results_url)\n",
    "\n",
    "accept_cookies_id, close_advert_id = \"onetrust-accept-btn-handler\", \"advertClose\"\n",
    "\n",
    "driver.find_element(By.ID, accept_cookies_id).click()\n",
    "driver.find_element(By.ID, close_advert_id).click()\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit and Implicit Waits\n",
    "With dynamic websites, things load in from a variety of different sources. We can't expect every item we want to show up onscreen immediately. However, we might know a generally expected amount of time we will need to guarantee an element to exist in the DOM. Therefore, we can use waits to specify how much we want to pause before trying to see if an element is there.\n",
    "\n",
    "Explicit - Wait a specific amount of time to find a certain element\n",
    "Implicit - When finding any element, wait a certain amount of time\n",
    "\n",
    "Use explicit - gives us more customization over the code, and can avoid problems from wait times being too great or too little on individual cases.\n",
    "\n",
    "### Expected Conditions\n",
    "\n",
    "This API can be used in conjuntion with waits - we wait EITHER for an expected condition to be true, or until the time limit is exceeded.\n",
    "\n",
    "Examples of Expected Conditions (EC):\n",
    "- title_is\n",
    "- title_contains\n",
    "- presence_of_element_located\n",
    "\n",
    "Below, we will use two waits, building on the last example. We simply wait for the presence of the elemnts we are trying to find, giving a 10 second buffer before an error is thrown. expected_conditions returns a boolean, and we can pass it into the .until() method of a WebDriverWait() object that if it evaluates to true, we will get the element returned, in which we can call a click().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = get_driver()\n",
    "driver.get(results_url)\n",
    "\n",
    "accept_cookies_id, close_advert_id, invalid_element_id = \"onetrust-accept-btn-handler\", \"advertClose\", \"not-an-element\"\n",
    "\n",
    "try:\n",
    "    accept_cookies_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, accept_cookies_id))\n",
    "    ).click()\n",
    "\n",
    "    close_advert_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, close_advert_id))\n",
    "    ).click()\n",
    "\n",
    "    # THE CODE BELOW WILL NOT WORK\n",
    "    # Keep it uncommented to see how an invalid element will fail with this code\n",
    "    # invalid_element = WebDriverWait(driver, 10).until(\n",
    "    #     EC.presence_of_element_located((By.ID, invalid_element_id))\n",
    "    # ).click()\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XPaths\n",
    "We can also parse through the DOM with XPaths. XPaths give us a different, but similar, way of parsing HTML data. XPaths are intended to be used with XML, but can often adequately work with HTML pages. We will not go extremely in depth to these concepts here, but you can read more about them below. We will use one XPath example in our next codeblock.\n",
    "\n",
    "More information: \n",
    "- https://www.w3schools.com/xml/xpath_intro.asp\n",
    "- https://scrapfly.io/blog/parsing-html-with-xpath/\n",
    "\n",
    "### Scrolling using ActionChains\n",
    "We can use the ActionChains library for various actions in the browser. In this case, we will pass the object returned by WebDriverWait to ActionChains in order to scroll to it, in turn loading the rest of the content on the page.\n",
    "\n",
    "We will use XPath to check for the existence of an element containing the text of the date that we want to scroll down to. Once we confirm this is true, we can scrape the HTML data from the page, close the browser, and parse as we did previously with BeautifulSoup, eventually creating a Pandas dataframe that is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date           home            away home_score   \n",
      "0     Tuesday 20 February 2024       Man City       Brentford          1  \\\n",
      "1      Monday 19 February 2024        Everton  Crystal Palace          1   \n",
      "2      Sunday 18 February 2024  Sheffield Utd        Brighton          0   \n",
      "3      Sunday 18 February 2024          Luton         Man Utd          1   \n",
      "4    Saturday 17 February 2024      Brentford       Liverpool          1   \n",
      "..                         ...            ...             ...        ...   \n",
      "244    Saturday 12 August 2023       Brighton           Luton          4   \n",
      "245    Saturday 12 August 2023        Everton          Fulham          0   \n",
      "246    Saturday 12 August 2023  Sheffield Utd  Crystal Palace          0   \n",
      "247    Saturday 12 August 2023      Newcastle     Aston Villa          5   \n",
      "248      Friday 11 August 2023        Burnley        Man City          0   \n",
      "\n",
      "    away_score                             stadium  \n",
      "0            0          Etihad Stadium, Manchester  \n",
      "1            1            Goodison Park, Liverpool  \n",
      "2            5             Bramall Lane, Sheffield  \n",
      "3            2              Kenilworth Road, Luton  \n",
      "4            4  Gtech Community Stadium, Brentford  \n",
      "..         ...                                 ...  \n",
      "244          1    American Express Stadium, Falmer  \n",
      "245          1            Goodison Park, Liverpool  \n",
      "246          1             Bramall Lane, Sheffield  \n",
      "247          1          St. James' Park, Newcastle  \n",
      "248          3                  Turf Moor, Burnley  \n",
      "\n",
      "[249 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "driver = get_driver()\n",
    "driver.get(results_url)\n",
    "\n",
    "accept_cookies_id, close_advert_id = \"onetrust-accept-btn-handler\", \"advertClose\"\n",
    "\n",
    "try:\n",
    "    accept_cookies_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, accept_cookies_id))\n",
    "    ).click()\n",
    "\n",
    "    close_advert_element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, close_advert_id))\n",
    "    ).click()\n",
    "\n",
    "    # Scroll to footer to activate JavaScript load of data\n",
    "    ActionChains(driver).scroll_to_element(\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"footer\"))\n",
    "        )\n",
    "    ).perform()\n",
    "\n",
    "    # waiting until all the data has loaded onto the page\n",
    "    date_string_to_find = \"Friday 11 August 2023\"\n",
    "    date_xpath = f\"//*[contains(text(),'{date_string_to_find}')]\"\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, date_xpath))\n",
    "    )\n",
    "\n",
    "    # Once we fetch the HTML, we can use beautifulsoup to parse through the webpage\n",
    "    pl_html = driver.find_element(By.TAG_NAME, \"html\").get_attribute(\"innerHTML\")\n",
    "    driver.quit()\n",
    "    \n",
    "    soup = bs(pl_html, \"html.parser\")\n",
    "\n",
    "    dates, home, away, home_score, away_score, stadium = [], [], [], [], [], []\n",
    "    col_names = \"date\", \"home\", \"away\", \"home_score\", \"away_score\", \"stadium\"\n",
    "\n",
    "    for date in soup.select(\".fixtures__date-container\"):\n",
    "        match_date = date.find(\"time\").text\n",
    "        for _ in range(len(date.select(\".match-fixture\"))):\n",
    "            dates.append(match_date)\n",
    "\n",
    "    match_list= [i for i in soup.select(\".matchList > .match-fixture\")]\n",
    "    home = [i['data-home'] for i in match_list]\n",
    "    away = [i['data-away'] for i in match_list]\n",
    "    scores = [i.text.split(\"-\") for i in soup.select(\".match-fixture__score\")]\n",
    "    home_score = [score[0] for score in scores]\n",
    "    away_score = [score[1] for score in scores]\n",
    "    stadium = [i['data-venue'] for i in match_list]\n",
    "\n",
    "    cols = [dates, home, away, home_score, away_score, stadium]\n",
    "    data = dict()\n",
    "    for i in range(len(col_names)):\n",
    "        data[col_names[i]] = cols[i]\n",
    "\n",
    "    df = pd.DataFrame(data=data)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(\"There was an error.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium is a powerful library that we only scratched the surface of today. You can do other actions such as run JavaScript in the browser, simulate keyboard and mouse input, and interact with a wide variety of dynamic elements on the page. Your specific use case will determine what you need to use, but many of the concepts used in this notebook, although simple, can get you through most actions needed to retrieve data from a dynamic webpage.\n",
    "\n",
    "For more information:\n",
    "- https://selenium-python.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
